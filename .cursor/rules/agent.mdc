---
alwaysApply: true
---

# Automaton Auditor — Agent Rules

## What This Project Is

**Automaton Auditor** is an automated quality-assurance swarm that audits GitHub repositories and PDF reports. It uses a **Digital Courtroom** architecture:

- **Input:** A GitHub repo URL and a PDF report.
- **Process:** Detectives (RepoInvestigator, DocAnalyst, VisionInspector) collect forensic evidence in parallel → Judges (Prosecutor, Defense, Tech Lead) deliberate per rubric criterion in parallel → Chief Justice synthesizes a verdict with hardcoded rules.
- **Output:** A production-grade Markdown audit report (Executive Summary, Criterion Breakdown, Remediation Plan).

The stack is **Python 3.11+**, **uv**, **LangGraph**, **LangChain**, **Pydantic**; state is typed with reducers for parallel nodes; rubric is loaded from JSON. See `docs/automaton_auditor_challenge description.md` for the original challenge.

---

## Specs: Where to Look and When

All authoritative specs live in **`specs/`**. Follow them for requirements, architecture, contracts, and conventions.

| File | What it does | When to check |
|------|----------------|---------------|
| **`specs/system_requirement_spec.md`** | System requirements (shall-statements). Source of truth for *what* the system must do. | Before designing or implementing any feature; when in doubt about a requirement. |
| **`specs/_meta.md`** | Technical standards, architectural style, tech stack, API patterns, file layout (e.g. `src/state.py`, `src/tools/`, `src/nodes/`, `src/graph.py`). | When choosing technologies, patterns, or file locations; when writing new modules. |
| **`specs/system_architecture.md`** | Component structure, state model, runtime flow (Detective → Judicial → Supreme Court), node topology, data flow. | When adding or changing nodes, state fields, or graph structure. |
| **`specs/api_contracts.md`** | Contracts for entry API, state/data types (Evidence, JudicialOpinion, AuditReport, AgentState), node signature, tools (repo, doc, vision), rubric JSON schema, report Markdown structure, env vars. | When implementing or changing any API, type, tool, rubric shape, or report format. |

**Rule:** Before implementing, read the relevant spec sections. When a spec says "shall," implementation **must** comply. Do not add requirements or contracts that contradict the specs.

---

## Implementation Workflow: Plan → Code → Review

For **every** implementation (feature, fix, or refactor), follow this cycle:

### 1. Plan

- **Read specs:** Check `specs/` as needed (see table above). Identify which SRS requirements, architecture elements, and API contracts apply.
- **Read the implementation plan:** Open **`docs/implementation_plan.md`**. Determine which phase(s) your work belongs to and which deliverables/tests are relevant.
- **Confirm scope:** List the files you will create or change and the acceptance criteria (from implementation plan or specs). If the task spans phases, note dependencies and test strategy.
- **Update the implementation plan (mandatory):** Before or while coding, **check and update `docs/implementation_plan.md`**:
  - Mark completed deliverables (checkboxes) when you finish them.
  - Add or adjust phases/deliverables if the plan no longer matches reality (e.g. new sub-tasks, changed order).
  - Keep "How to Test" and "Exit Criteria" accurate so the next agent or developer can verify the phase.

### 2. Code

- **Follow specs:** Implement to satisfy `system_requirement_spec.md`, `_meta.md`, `system_architecture.md`, and `api_contracts.md`. Use typed state (Pydantic/TypedDict), reducers for parallel-written state, sandboxed repo tools (no `os.system` with unsanitized input), structured Judge output (`.with_structured_output(JudicialOpinion)`), and rubric-driven behavior.
- **Stick to layout:** Use `src/state.py`, `src/tools/repo_tools.py`, `src/tools/doc_tools.py`, `src/nodes/detectives.py`, `src/nodes/judges.py`, `src/nodes/justice.py`, `src/graph.py` as in _meta and architecture.
- **Test as you go:** Prefer tests that match the implementation plan’s "How to Test" for the phase (unit tests for tools, graph runs for nodes, full run for end-to-end).

### 3. Review

- **Self-check against specs:** Verify the change meets the relevant SRS, architecture, and API contract clauses. Fix any violation (e.g. missing reducer, wrong type, wrong report structure).
- **Update implementation plan again:** After implementation, **re-check `docs/implementation_plan.md`**: ensure all completed work is reflected (checkboxes, phase status) and that the plan is still the single place to see "what’s done and what’s next."
- **Document if needed:** If you added env vars, CLI options, or new modules, update README and `.env.example` so the next agent can run and test correctly.

---

## Mandatory: Implementation Plan Check and Update

- **Every time** you work on an implementation (new feature, bugfix, or refactor that touches deliverables):
  1. **Check** `docs/implementation_plan.md` at the start to see current phase, deliverables, and tests.
  2. **Update** `docs/implementation_plan.md` when you complete deliverables (check off items), add sub-tasks, or change exit criteria so the plan stays accurate.
- Treat the implementation plan as the **living checklist** for the project. A new agent should be able to open it and immediately see what is done and what to do next.

---

## Quick Reference

- **Requirements (what to build):** `specs/system_requirement_spec.md`
- **Standards and patterns (how to build):** `specs/_meta.md`
- **Structure and flow (where things go):** `specs/system_architecture.md`
- **Types, APIs, contracts (interfaces):** `specs/api_contracts.md`
- **What to do next and how to test it:** `docs/implementation_plan.md` — **check and update every time.**

Plan → Code → Review. Always align with specs and keep the implementation plan current.
